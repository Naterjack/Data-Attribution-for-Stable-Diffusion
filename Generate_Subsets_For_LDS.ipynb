{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the subsets of the dataset for calculating the LDS\n",
    "To calculate the LDS (Linear Datamodeling Score), we need a large number of diffusion models each trained over different subsets of the whole dataset (in our case, CIFAR-10).\n",
    "\n",
    "DTRAK uses 64 subsets, each covering 50% of the orignal dataset, then fine tunes 9 stable diffusion models on each subset. Unfortunately this is infeasible for our hardware, so we instead use 32 subsets of 50% coverage, and train a single diffusion model on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joy/miniconda3/envs/SD3HF_Unstable/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/joy/miniconda3/envs/SD3HF_Unstable/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/joy/miniconda3/envs/SD3HF_Unstable/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    }
   ],
   "source": [
    "from utils.config import CIFAR_10_Config, Project_Config\n",
    "DATASET_NAME = \"cifar-10\"\n",
    "dataset_config = CIFAR_10_Config()\n",
    "project_config = Project_Config()\n",
    "\n",
    "SAVE_ORIGINAL_IMAGES_TO_DISK = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_config.dataset.save_to_disk(\"./datasets/cifar-10/all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows per class per subset 2500\n"
     ]
    }
   ],
   "source": [
    "NUM_SUBSETS = 32\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "ROWS_PER_CLASS = int(dataset_config.dataset.num_rows/ NUM_CLASSES)\n",
    "\n",
    "SUBSET_SIZE_ALPHA = 0.5\n",
    "SUBSET_SIZE_TOTAL = int(dataset_config.dataset.num_rows * SUBSET_SIZE_ALPHA)\n",
    "ROWS_PER_CLASS_SUBSET = int(ROWS_PER_CLASS*SUBSET_SIZE_ALPHA)\n",
    "\n",
    "assert(SUBSET_SIZE_TOTAL == ROWS_PER_CLASS_SUBSET * NUM_CLASSES)\n",
    "print(f\"Rows per class per subset {ROWS_PER_CLASS_SUBSET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config.dataset = dataset_config.dataset.sort(column_names=dataset_config.caption_column)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "base_path = (\".\" + project_config.folder_symbol +\n",
    "             \"datasets\" + project_config.folder_symbol +\n",
    "             DATASET_NAME + project_config.folder_symbol)\n",
    "\n",
    "base_path_images = (base_path + project_config.folder_symbol +\n",
    "                    \"train\" + project_config.folder_symbol)\n",
    "\n",
    "Path(base_path_images).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for class_caption in dataset_config.class_captions:\n",
    "    Path(base_path_images+class_caption+project_config.folder_symbol).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if SAVE_ORIGINAL_IMAGES_TO_DISK:\n",
    "    i = 0\n",
    "    for item in dataset_config.dataset:\n",
    "        item[dataset_config.image_column].save(f\"{base_path_images}{item[dataset_config.caption_column]}{project_config.folder_symbol}{i}.png\")\n",
    "        i = i+1\n",
    "        if i >= ROWS_PER_CLASS:\n",
    "            i=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "\n",
    "#https://huggingface.co/docs/hub/en/datasets-manual-configuration\n",
    "\"\"\"\n",
    "---\n",
    "configs:\n",
    "- config_name: subset_0\n",
    "  data_files:\n",
    "  - split: train\n",
    "    path: \n",
    "    - \"data/*.csv\"\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "file_lines = []\n",
    "file_lines.append(\"---\")\n",
    "file_lines.append(\"configs:\")\n",
    "\n",
    "for subset in range(NUM_SUBSETS):\n",
    "    file_lines.append(f\"  - config_name: subset_{subset}\")\n",
    "    file_lines.append(\"    drop_labels: false\")\n",
    "    file_lines.append(\"    data_files:\")\n",
    "    file_lines.append(\"      - split: train\")\n",
    "    file_lines.append(\"        path:\")\n",
    "    for class_caption in dataset_config.class_captions:\n",
    "        class_i = rng.choice(ROWS_PER_CLASS,ROWS_PER_CLASS_SUBSET, replace=False)\n",
    "        for i in class_i:\n",
    "            file_lines.append(f\"          - \\\"train{project_config.folder_symbol}{class_caption}{project_config.folder_symbol}{i}.png\\\"\")\n",
    "\n",
    "\n",
    "file_lines.append(\"---\")\n",
    "\n",
    "for i in range(len(file_lines)):\n",
    "    file_lines[i] = file_lines[i] + \"\\n\"\n",
    "\n",
    "print(file_lines[0])\n",
    "\n",
    "f = open(base_path+\"README.md\", \"w\")\n",
    "f.writelines(file_lines)\n",
    "f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 25000/25000 [00:00<00:00, 60452.37files/s] \n",
      "Generating train split: 25000 examples [00:00, 26216.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "test_ds = datasets.load_dataset(base_path, name=\"subset_0\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'], id=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds.features['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor subset in range(NUM_SUBSETS):\\n    subset_indices = np.zeros(SUBSET_SIZE_TOTAL, np.int32)\\n\\n    for i in range(NUM_CLASSES):\\n        class_i = rng.choice(ROWS_PER_CLASS,ROWS_PER_CLASS_SUBSET, replace=False)\\n        index_start = i*ROWS_PER_CLASS_SUBSET\\n        index_end = (i+1)*ROWS_PER_CLASS_SUBSET\\n        subset_indices[index_start:index_end] = class_i + (i*ROWS_PER_CLASS)\\n\\n    for i in range(NUM_CLASSES):\\n        assert(subset_indices[ROWS_PER_CLASS_SUBSET*i]>(ROWS_PER_CLASS*i))\\n\\n    dataset_subset = dataset_config.dataset.select(subset_indices)\\n    for class_caption in dataset_config.class_captions:\\n        assert(dataset_subset[dataset_config.caption_column].count(class_caption) == ROWS_PER_CLASS_SUBSET)\\n\\n    dataset_subset.save_to_disk(f\"./datasets/cifar-10/subset-{subset}\")\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This would make way more sense, but sadly the .arrow format that .save_to_disk produces \n",
    "#   is not supported by train_text_to_image\n",
    "\"\"\"\n",
    "for subset in range(NUM_SUBSETS):\n",
    "    subset_indices = np.zeros(SUBSET_SIZE_TOTAL, np.int32)\n",
    "\n",
    "    for i in range(NUM_CLASSES):\n",
    "        class_i = rng.choice(ROWS_PER_CLASS,ROWS_PER_CLASS_SUBSET, replace=False)\n",
    "        index_start = i*ROWS_PER_CLASS_SUBSET\n",
    "        index_end = (i+1)*ROWS_PER_CLASS_SUBSET\n",
    "        subset_indices[index_start:index_end] = class_i + (i*ROWS_PER_CLASS)\n",
    "\n",
    "    for i in range(NUM_CLASSES):\n",
    "        assert(subset_indices[ROWS_PER_CLASS_SUBSET*i]>(ROWS_PER_CLASS*i))\n",
    "\n",
    "    dataset_subset = dataset_config.dataset.select(subset_indices)\n",
    "    for class_caption in dataset_config.class_captions:\n",
    "        assert(dataset_subset[dataset_config.caption_column].count(class_caption) == ROWS_PER_CLASS_SUBSET)\n",
    "\n",
    "    dataset_subset.save_to_disk(f\"./datasets/cifar-10/subset-{subset}\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SD3HF_Unstable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
