{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is based on the sample code provided by traker at\n",
    "https://trak.readthedocs.io/en/latest/quickstart.html\n",
    "\n",
    "and the hugging face diffusers training code since their API does not let you get the loss directly\n",
    "https://github.com/huggingface/diffusers/blob/v0.30.3/examples/text_to_image/train_text_to_image.py\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_97130/3652397506.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_filename)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "CONVERT_SAFETENSORS_TO_CKPT = False\n",
    "CUDA = True\n",
    "\n",
    "#ckpt_files = list(Path('./checkpoints').rglob('*.pt'))\n",
    "#ckpts = [torch.load(ckpt, map_location='cpu') for ckpt in ckpt_files]\n",
    "\n",
    "filename = \"../FineTuned-SD1-cifar10/unet/diffusion_pytorch_model.safetensors\"\n",
    "#s/o https://gist.github.com/madaan/6c9be9613e6760b7dee79bdfa621fc0f\n",
    "if CONVERT_SAFETENSORS_TO_CKPT:\n",
    "    from safetensors.torch import load_file\n",
    "    ckpt = load_file(filename)\n",
    "    torch.save(ckpt, filename.replace(\".safetensors\", \".bin\"))\n",
    "\n",
    "ckpt_filename = filename.replace(\".safetensors\", \".bin\")\n",
    "ckpt = torch.load(ckpt_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joy/miniconda3/envs/SD3HF_Unstable/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/joy/miniconda3/envs/SD3HF_Unstable/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/joy/miniconda3/envs/SD3HF_Unstable/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Dataset({\n",
      "    features: ['img', 'label'],\n",
      "    num_rows: 50000\n",
      "})\n",
      "<class 'datasets.features.features.ClassLabel'>\n",
      "Dataset({\n",
      "    features: ['img', 'label_txt'],\n",
      "    num_rows: 50000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "IS_WINDOWS = False\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "from torchvision import transforms\n",
    "import os\n",
    "if IS_WINDOWS: \n",
    "    p = \"..\\\\FineTuned-SD1-cifar10\"\n",
    "else: #Assume UNIX-based\n",
    "    p = \"../FineTuned-SD1-cifar10\"\n",
    "print(os.path.isdir(p))\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    p, subfolder=\"tokenizer\",\n",
    ")\n",
    "\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    p, subfolder=\"text_encoder\",\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    p, subfolder=\"vae\",\n",
    ")\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "     p, subfolder=\"unet\",\n",
    ")\n",
    "\n",
    "text_encoder.to(\"cuda\")\n",
    "vae.to(\"cuda\")\n",
    "unet.to(\"cuda\")\n",
    "\n",
    "# Freeze vae and text_encoder\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "#unet.train()\n",
    "\n",
    "\n",
    "# Dataloader\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#https://huggingface.co/datasets/\n",
    "\n",
    "dataset_name = \"uoft-cs/cifar10\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "image_column = \"img\"\n",
    "caption_column = \"label_txt\"\n",
    "\n",
    "#dataset = dataset.rename_column(\"img\", \"image\")\n",
    "print(dataset)\n",
    "cl = dataset.features['label']\n",
    "print(type(cl))\n",
    "\n",
    "def convertLabel(x):\n",
    "    x['label_txt'] = cl.int2str(x['label'])\n",
    "    return x\n",
    "\n",
    "dataset = dataset.map(convertLabel)\n",
    "\n",
    "dataset = dataset.remove_columns(column_names=[\"label\"])\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# We need to tokenize input captions and transform the images.\n",
    "def tokenize_captions(examples, is_train=True):\n",
    "    inputs = tokenizer(\n",
    "        examples[caption_column], max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    return inputs.input_ids\n",
    "\n",
    "def preprocess_train(examples):\n",
    "        images = [image.convert(\"RGB\") for image in examples[image_column]]\n",
    "        examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n",
    "        examples[\"input_ids\"] = tokenize_captions(examples)\n",
    "        return examples\n",
    "\n",
    "#dataset = dataset.with_format(\"torch\")\n",
    "print(dataset)\n",
    "train_dataset = dataset.with_transform(preprocess_train)\n",
    "\n",
    "def collate_fn(examples):\n",
    "        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "        input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
    "        return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n",
    "\n",
    "# DataLoaders creation:\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:TRAK:Using ChunkedCudaProjector with2 chunks of sizes[518204160, 341316804].\n",
      "INFO:STORE:Existing model IDs in /home/joy/Lboro/CodeServer/git/Data-Attribution-for-Stable-Diffusion/TRAK/trak_results: [0]\n",
      "INFO:STORE:No model IDs in /home/joy/Lboro/CodeServer/git/Data-Attribution-for-Stable-Diffusion/TRAK/trak_results have been finalized.\n",
      "INFO:STORE:No existing TRAK scores in /home/joy/Lboro/CodeServer/git/Data-Attribution-for-Stable-Diffusion/TRAK/trak_results.\n"
     ]
    }
   ],
   "source": [
    "from trak import TRAKer\n",
    "from SD1ModelOutput import SD1ModelOutput\n",
    "\n",
    "traker = TRAKer(model=unet,\n",
    "                task=SD1ModelOutput,\n",
    "                train_set_size=len(train_dataloader.dataset),\n",
    "                proj_max_batch_size=8, #Default 32, requires an A100 apparently\n",
    "                #proj_dim=1024,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scoop whatever VRAM we can because this is going to be a tight fit\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joy/miniconda3/envs/SD3HF_Unstable/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2358: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::_scaled_dot_product_efficient_attention. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538438429/work/aten/src/ATen/functorch/BatchedFallback.cpp:81.)\n",
      "  hidden_states = F.scaled_dot_product_attention(\n",
      "/home/joy/miniconda3/envs/SD3HF_Unstable/lib/python3.10/site-packages/torch/autograd/graph.py:768: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::_scaled_dot_product_efficient_attention_backward. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538438429/work/aten/src/ATen/functorch/BatchedFallback.cpp:81.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m---> 41\u001b[0m     \u001b[43mtraker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeaturize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Tells TRAKer that we've given it all the information, at which point\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# TRAKer does some post-processing to get ready for the next step\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# (scoring target examples).\u001b[39;00m\n\u001b[1;32m     50\u001b[0m traker\u001b[38;5;241m.\u001b[39mfinalize_features()\n",
      "File \u001b[0;32m~/miniconda3/envs/SD3HF_Unstable/lib/python3.10/site-packages/trak/traker.py:428\u001b[0m, in \u001b[0;36mTRAKer.featurize\u001b[0;34m(self, batch, inds, num_samples)\u001b[0m\n\u001b[1;32m    425\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojector\u001b[38;5;241m.\u001b[39mproject(grads, model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaver\u001b[38;5;241m.\u001b[39mcurrent_model_id)\n\u001b[1;32m    426\u001b[0m grads \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_factor\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaver\u001b[38;5;241m.\u001b[39mcurrent_store[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrads\u001b[39m\u001b[38;5;124m\"\u001b[39m][inds] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 428\u001b[0m     \u001b[43mgrads\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    429\u001b[0m )\n\u001b[1;32m    431\u001b[0m loss_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_computer\u001b[38;5;241m.\u001b[39mcompute_loss_grad(batch)\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaver\u001b[38;5;241m.\u001b[39mcurrent_store[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_to_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m][inds] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    433\u001b[0m     loss_grads\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    434\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#for model_id, ckpt in enumerate(ckpts): #tqdm\n",
    "\n",
    "# TRAKer loads the provided checkpoint and also associates\n",
    "\n",
    "# the provided (unique) model_id with the checkpoint.\n",
    "model_id = 0\n",
    "traker.load_checkpoint(ckpt, model_id=model_id)\n",
    "\n",
    "weight_dtype = torch.float32\n",
    "i = 0\n",
    "for batch in train_dataloader:\n",
    "    if CUDA:\n",
    "        batch = [x.cuda() for y,x in batch.items()]\n",
    "    else:\n",
    "        batch = [x for y,x in batch.items()]\n",
    "    image = batch[0]\n",
    "    tokens = batch[1]\n",
    "    # Convert images to latent space\n",
    "    latents = vae.encode(image.to(weight_dtype)).latent_dist.sample()\n",
    "    latents = latents * vae.config.scaling_factor\n",
    "    encoder_hidden_states = text_encoder(tokens, return_dict=False)[0]\n",
    "\n",
    "    batch = [latents, encoder_hidden_states]\n",
    "\n",
    "    #batch = [x.cuda() for y,x in batch.items()]\n",
    "\n",
    "    # TRAKer computes features corresponding to the batch of examples,\n",
    "    # using the checkpoint loaded above.\n",
    "\n",
    "    #pipe.vae.to(\"cpu\")\n",
    "    #pipe.text_encoder.to(\"cpu\")\n",
    "    i += 1\n",
    "    print(i)\n",
    "    traker.featurize(batch=batch, num_samples=batch[0].shape[0])\n",
    "\n",
    "\n",
    "# Tells TRAKer that we've given it all the information, at which point\n",
    "\n",
    "# TRAKer does some post-processing to get ready for the next step\n",
    "\n",
    "# (scoring target examples).\n",
    "\n",
    "traker.finalize_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_targets = get_dataloader(batch_size=batch_size, split='val', augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_id, ckpt in enumerate(tqdm(ckpts)):\n",
    "\n",
    "    traker.start_scoring_checkpoint(exp_name='quickstart',\n",
    "\n",
    "                                    checkpoint=ckpt,\n",
    "\n",
    "                                    model_id=model_id,\n",
    "\n",
    "                                    num_targets=len(loader_targets.dataset))\n",
    "\n",
    "    for batch in loader_targets:\n",
    "\n",
    "        batch = [x.cuda() for x in batch]\n",
    "\n",
    "        traker.score(batch=batch, num_samples=batch[0].shape[0])\n",
    "\n",
    "\n",
    "scores = traker.finalize_scores(exp_name='quickstart')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SD3HF_Unstable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
